#!/usr/bin/env python3
"""
Ultimate MoE Solution - Complete End-to-End Integration Testing Suite
Tests all phases (1, 2, 3) including integration of all components, dashboards, 
learning system, and deployment modules.
"""

import asyncio
import time
import json
import os
from typing import Dict, Any, List
from datetime import datetime
import traceback

# Import all system components
from high_performance_system.core.ultimate_moe_system import UltimateMoESystem
from high_performance_system.core.advanced_expert_ensemble import AdvancedExpertEnsemble
from high_performance_system.core.intelligent_domain_router import IntelligentDomainRouter
from high_performance_system.core.enhanced_dataset_profiler import EnhancedDatasetProfiler
from high_performance_system.core.comprehensive_pii_detector import ComprehensivePIIDetector
from high_performance_system.core.advanced_trust_scorer import AdvancedTrustScorer

# Import Phase 2 components
from high_performance_system.core.enhanced_rag_pipeline import EnhancedRAGPipeline
from high_performance_system.core.advanced_multi_agent_system import AdvancedMultiAgentSystem
from high_performance_system.core.uncertainty_aware_system import UncertaintyAwareSystem
from high_performance_system.core.performance_optimizer import AdvancedPerformanceOptimizer

# Import Phase 3 components
from high_performance_system.analytics.ultimate_analytics_dashboard import UltimateAnalyticsDashboard
from high_performance_system.analytics.sme_dashboard import AdvancedSMEDashboard
from high_performance_system.learning.continuous_learning_system import ContinuousLearningSystem
from high_performance_system.deployment.production_deployer import ProductionDeployer

class UltimateMoECompleteIntegrationTester:
    """Comprehensive end-to-end tester for all Ultimate MoE Solution components"""
    
    def __init__(self):
        self.test_results = {}
        self.start_time = time.time()
        self.test_data = self._generate_test_data()
        
        # Initialize all system components
        self._initialize_components()
        
    def _initialize_components(self):
        """Initialize all system components"""
        try:
            # Phase 1 components
            self.moe_system = UltimateMoESystem()
            self.expert_ensemble = AdvancedExpertEnsemble()
            self.domain_router = IntelligentDomainRouter()
            self.dataset_profiler = EnhancedDatasetProfiler()
            self.pii_detector = ComprehensivePIIDetector()
            self.trust_scorer = AdvancedTrustScorer()
            
            # Phase 2 components
            self.rag_pipeline = EnhancedRAGPipeline()
            self.multi_agent = AdvancedMultiAgentSystem()
            self.uncertainty_system = UncertaintyAwareSystem()
            self.performance_optimizer = AdvancedPerformanceOptimizer()
            
            # Phase 3 components
            self.analytics_dashboard = UltimateAnalyticsDashboard()
            self.sme_dashboard = AdvancedSMEDashboard()
            self.learning_system = ContinuousLearningSystem()
            self.production_deployer = ProductionDeployer()
            
        except Exception as e:
            print(f"âŒ Error initializing components: {e}")
            raise
    
    def _generate_test_data(self) -> Dict[str, Any]:
        """Generate comprehensive test data"""
        return {
            'test_texts': [
                "The COVID-19 pandemic has accelerated digital transformation across industries, with remote work becoming the new normal for many organizations worldwide.",
                "Machine learning algorithms can achieve 95% accuracy in medical diagnosis, outperforming human doctors in certain specialized areas.",
                "Climate change is causing unprecedented global temperature increases, with the last decade being the warmest on record according to NASA data.",
                "The global economy is expected to grow by 3.5% in 2024, driven by technological innovation and sustainable development initiatives.",
                "Artificial intelligence will create more jobs than it eliminates, with estimates suggesting 2.3 million new jobs by 2030."
            ],
            'test_queries': [
                "What are the latest developments in quantum computing?",
                "How does blockchain technology work in banking?",
                "What are the health benefits of Mediterranean diet?",
                "Explain the legal implications of AI in healthcare",
                "What are the environmental impacts of renewable energy?"
            ],
            'test_documents': [
                "Quantum computing represents a paradigm shift in computational power. Recent developments include superconducting qubits achieving 99.9% fidelity and quantum supremacy demonstrations by Google and IBM.",
                "Blockchain technology in banking enables secure, transparent, and immutable transaction records. It reduces fraud, lowers costs, and enables real-time settlement across borders.",
                "The Mediterranean diet emphasizes fruits, vegetables, whole grains, and healthy fats. Research shows it reduces heart disease risk by 30% and improves cognitive function.",
                "AI in healthcare raises legal concerns about liability, privacy, and regulatory compliance. The FDA has approved over 500 AI medical devices, but legal frameworks are evolving.",
                "Renewable energy reduces greenhouse gas emissions by 80-90% compared to fossil fuels. Solar and wind power are now cost-competitive and create more jobs than fossil fuel industries."
            ]
        }
    
    async def test_phase1_integration(self) -> Dict[str, Any]:
        """Test Phase 1 components integration"""
        print("ğŸ”§ Testing Phase 1 Integration...")
        
        results = {
            "phase": "Phase 1 - Core Enhancement",
            "components": {},
            "integration_tests": [],
            "performance": {},
            "errors": []
        }
        
        try:
            # Test 1: Complete MoE System verification
            start_time = time.time()
            verification_result = await self.moe_system.verify_text(
                text=self.test_data['test_texts'][0],
                context="Integration test",
                enable_advanced_features=True
            )
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Complete MoE System Verification",
                "status": "PASS" if verification_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(verification_result.keys()) if verification_result else []
            })
            
            # Test 2: Expert Ensemble with all experts
            start_time = time.time()
            ensemble_result = await self.expert_ensemble.verify_with_all_experts(
                text=self.test_data['test_texts'][1],
                context="Expert ensemble test"
            )
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Expert Ensemble Verification",
                "status": "PASS" if ensemble_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "expert_count": len(ensemble_result) if ensemble_result else 0
            })
            
            # Test 3: Intelligent Domain Routing
            start_time = time.time()
            routing_result = await self.domain_router.route_to_experts(
                text=self.test_data['test_texts'][2],
                context="Routing test"
            )
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Intelligent Domain Routing",
                "status": "PASS" if routing_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "routing_confidence": routing_result.get('confidence', 0) if routing_result else 0
            })
            
            # Test 4: Cleanlab Integration (Dataset Profiler + PII + Trust)
            start_time = time.time()
            
            # Dataset profiling
            profile_result = await self.dataset_profiler.profile_text(self.test_data['test_texts'][3])
            
            # PII detection
            pii_result = await self.pii_detector.detect_pii(self.test_data['test_texts'][3])
            
            # Trust scoring
            trust_result = await self.trust_scorer.score_trust(
                text=self.test_data['test_texts'][3],
                context="Cleanlab integration test"
            )
            
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Cleanlab Integration (Profiler + PII + Trust)",
                "status": "PASS" if all([profile_result, pii_result, trust_result]) else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "components": ["profiler", "pii_detector", "trust_scorer"]
            })
            
            # Performance metrics
            total_latency = sum(test["latency"] for test in results["integration_tests"])
            results["performance"] = {
                "total_latency_ms": total_latency,
                "average_latency_ms": total_latency / len(results["integration_tests"]),
                "tests_passed": len([t for t in results["integration_tests"] if t["status"] == "PASS"]),
                "tests_failed": len([t for t in results["integration_tests"] if t["status"] == "FAIL"])
            }
            
        except Exception as e:
            results["errors"].append(f"Phase 1 Integration Error: {str(e)}")
            print(f"âŒ Phase 1 Integration Error: {e}")
        
        return results
    
    async def test_phase2_integration(self) -> Dict[str, Any]:
        """Test Phase 2 components integration"""
        print("ğŸš€ Testing Phase 2 Integration...")
        
        results = {
            "phase": "Phase 2 - Advanced Features",
            "components": {},
            "integration_tests": [],
            "performance": {},
            "errors": []
        }
        
        try:
            # Test 1: Enhanced RAG Pipeline
            start_time = time.time()
            rag_result = await self.rag_pipeline.enhanced_rag_with_moe(
                query=self.test_data['test_queries'][0],
                documents=self.test_data['test_documents']
            )
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Enhanced RAG Pipeline",
                "status": "PASS" if rag_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(rag_result.keys()) if rag_result else []
            })
            
            # Test 2: Multi-Agent System
            start_time = time.time()
            agent_result = await self.multi_agent.comprehensive_evaluation(
                text=self.test_data['test_texts'][0],
                context="Multi-agent integration test"
            )
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Multi-Agent System",
                "status": "PASS" if agent_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(agent_result.keys()) if agent_result else []
            })
            
            # Test 3: Uncertainty-Aware System
            start_time = time.time()
            uncertainty_result = await self.uncertainty_system.comprehensive_uncertainty_analysis(
                text=self.test_data['test_texts'][1]
            )
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Uncertainty-Aware System",
                "status": "PASS" if uncertainty_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(uncertainty_result.keys()) if uncertainty_result else []
            })
            
            # Test 4: Performance Optimizer
            start_time = time.time()
            perf_result = await self.performance_optimizer.optimize_system_performance()
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Performance Optimizer",
                "status": "PASS" if perf_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(perf_result.keys()) if perf_result else []
            })
            
            # Performance metrics
            total_latency = sum(test["latency"] for test in results["integration_tests"])
            results["performance"] = {
                "total_latency_ms": total_latency,
                "average_latency_ms": total_latency / len(results["integration_tests"]),
                "tests_passed": len([t for t in results["integration_tests"] if t["status"] == "PASS"]),
                "tests_failed": len([t for t in results["integration_tests"] if t["status"] == "FAIL"])
            }
            
        except Exception as e:
            results["errors"].append(f"Phase 2 Integration Error: {str(e)}")
            print(f"âŒ Phase 2 Integration Error: {e}")
        
        return results
    
    async def test_phase3_integration(self) -> Dict[str, Any]:
        """Test Phase 3 components integration"""
        print("ğŸ­ Testing Phase 3 Integration...")
        
        results = {
            "phase": "Phase 3 - Production Ready",
            "components": {},
            "integration_tests": [],
            "performance": {},
            "errors": []
        }
        
        try:
            # Test 1: Analytics Dashboard
            start_time = time.time()
            # Test dashboard initialization and data loading
            dashboard_initialized = hasattr(self.analytics_dashboard, 'performance_history')
            dashboard_data_loaded = len(self.analytics_dashboard.performance_history) > 0
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Analytics Dashboard",
                "status": "PASS" if dashboard_initialized and dashboard_data_loaded else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "data_points": len(self.analytics_dashboard.performance_history)
            })
            
            # Test 2: SME Dashboard
            start_time = time.time()
            sme_initialized = hasattr(self.sme_dashboard, 'render_sme_dashboard')
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "SME Dashboard",
                "status": "PASS" if sme_initialized else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2)
            })
            
            # Test 3: Continuous Learning System
            start_time = time.time()
            learning_result = await self.learning_system.run_learning_cycle()
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Continuous Learning System",
                "status": "PASS" if learning_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(learning_result.keys()) if learning_result else []
            })
            
            # Test 4: Production Deployment
            start_time = time.time()
            deployment_result = await self.production_deployer.deploy_to_production()
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Production Deployment",
                "status": "PASS" if deployment_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(deployment_result.keys()) if deployment_result else []
            })
            
            # Performance metrics
            total_latency = sum(test["latency"] for test in results["integration_tests"])
            results["performance"] = {
                "total_latency_ms": total_latency,
                "average_latency_ms": total_latency / len(results["integration_tests"]),
                "tests_passed": len([t for t in results["integration_tests"] if t["status"] == "PASS"]),
                "tests_failed": len([t for t in results["integration_tests"] if t["status"] == "FAIL"])
            }
            
        except Exception as e:
            results["errors"].append(f"Phase 3 Integration Error: {str(e)}")
            print(f"âŒ Phase 3 Integration Error: {e}")
        
        return results
    
    async def test_cross_phase_integration(self) -> Dict[str, Any]:
        """Test integration across all phases"""
        print("ğŸ”— Testing Cross-Phase Integration...")
        
        results = {
            "phase": "Cross-Phase Integration",
            "integration_tests": [],
            "performance": {},
            "errors": []
        }
        
        try:
            # Test 1: Phase 1 + Phase 2 Integration
            start_time = time.time()
            
            # Use Phase 1 MoE system with Phase 2 RAG
            moe_result = await self.moe_system.verify_text(
                text=self.test_data['test_texts'][0],
                context="Cross-phase test"
            )
            
            # Use RAG results in multi-agent system
            if moe_result and 'final_verification' in moe_result:
                agent_result = await self.multi_agent.comprehensive_evaluation(
                    text=str(moe_result['final_verification']),
                    context="Cross-phase integration"
                )
            else:
                agent_result = None
            
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Phase 1 + Phase 2 Integration (MoE + RAG + Multi-Agent)",
                "status": "PASS" if agent_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(agent_result.keys()) if agent_result else []
            })
            
            # Test 2: Phase 2 + Phase 3 Integration
            start_time = time.time()
            
            # Use uncertainty analysis results in learning system
            uncertainty_result = await self.uncertainty_system.comprehensive_uncertainty_analysis(
                text=self.test_data['test_texts'][1]
            )
            
            if uncertainty_result:
                learning_data = {
                    'uncertainty_analysis': uncertainty_result,
                    'performance': {
                        'accuracy': 97.5,
                        'latency': 15,
                        'throughput': 400
                    }
                }
                learning_result = await self.learning_system.update_system_knowledge(learning_data)
            else:
                learning_result = None
            
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Phase 2 + Phase 3 Integration (Uncertainty + Learning)",
                "status": "PASS" if learning_result else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "result_keys": list(learning_result.keys()) if learning_result else []
            })
            
            # Test 3: Complete End-to-End Integration
            start_time = time.time()
            
            # Complete pipeline: Text â†’ MoE â†’ RAG â†’ Multi-Agent â†’ Uncertainty â†’ Learning â†’ Analytics
            text = self.test_data['test_texts'][2]
            
            # Step 1: MoE verification
            moe_verification = await self.moe_system.verify_text(text, context="E2E test")
            
            # Step 2: RAG enhancement
            rag_enhancement = await self.rag_pipeline.enhanced_rag_with_moe(
                query=text,
                documents=self.test_data['test_documents']
            )
            
            # Step 3: Multi-agent evaluation
            agent_evaluation = await self.multi_agent.comprehensive_evaluation(
                text=text,
                context="E2E integration"
            )
            
            # Step 4: Uncertainty analysis
            uncertainty_analysis = await self.uncertainty_system.comprehensive_uncertainty_analysis(text)
            
            # Step 5: Learning update
            learning_update = await self.learning_system.update_system_knowledge({
                'moe_result': moe_verification,
                'rag_result': rag_enhancement,
                'agent_result': agent_evaluation,
                'uncertainty_result': uncertainty_analysis
            })
            
            end_time = time.time()
            
            results["integration_tests"].append({
                "test": "Complete End-to-End Integration (All Phases)",
                "status": "PASS" if all([moe_verification, rag_enhancement, agent_evaluation, uncertainty_analysis, learning_update]) else "FAIL",
                "latency": round((end_time - start_time) * 1000, 2),
                "components": ["MoE", "RAG", "Multi-Agent", "Uncertainty", "Learning"]
            })
            
            # Performance metrics
            total_latency = sum(test["latency"] for test in results["integration_tests"])
            results["performance"] = {
                "total_latency_ms": total_latency,
                "average_latency_ms": total_latency / len(results["integration_tests"]),
                "tests_passed": len([t for t in results["integration_tests"] if t["status"] == "PASS"]),
                "tests_failed": len([t for t in results["integration_tests"] if t["status"] == "FAIL"])
            }
            
        except Exception as e:
            results["errors"].append(f"Cross-Phase Integration Error: {str(e)}")
            print(f"âŒ Cross-Phase Integration Error: {e}")
        
        return results
    
    async def run_complete_integration_test(self) -> Dict[str, Any]:
        """Run complete end-to-end integration test"""
        print("ğŸ† Starting Ultimate MoE Complete Integration Testing Suite")
        print("=" * 80)
        
        # Run all integration tests
        phase1_results = await self.test_phase1_integration()
        phase2_results = await self.test_phase2_integration()
        phase3_results = await self.test_phase3_integration()
        cross_phase_results = await self.test_cross_phase_integration()
        
        # Compile overall results
        all_results = {
            "test_suite": "Ultimate MoE Complete Integration",
            "timestamp": datetime.now().isoformat(),
            "phases": {
                "phase1": phase1_results,
                "phase2": phase2_results,
                "phase3": phase3_results,
                "cross_phase": cross_phase_results
            }
        }
        
        # Calculate overall statistics
        total_tests = 0
        total_passed = 0
        total_failed = 0
        total_latency = 0
        total_errors = 0
        
        for phase_name, phase_results in all_results["phases"].items():
            if "integration_tests" in phase_results:
                phase_tests = len(phase_results["integration_tests"])
                phase_passed = len([t for t in phase_results["integration_tests"] if t["status"] == "PASS"])
                phase_failed = len([t for t in phase_results["integration_tests"] if t["status"] == "FAIL"])
                phase_latency = sum(t["latency"] for t in phase_results["integration_tests"])
                phase_errors = len(phase_results.get("errors", []))
                
                total_tests += phase_tests
                total_passed += phase_passed
                total_failed += phase_failed
                total_latency += phase_latency
                total_errors += phase_errors
        
        all_results["summary"] = {
            "total_tests": total_tests,
            "total_passed": total_passed,
            "total_failed": total_failed,
            "success_rate": round((total_passed / total_tests) * 100, 2) if total_tests > 0 else 0,
            "total_latency_ms": total_latency,
            "average_latency_ms": round(total_latency / total_tests, 2) if total_tests > 0 else 0,
            "total_errors": total_errors,
            "test_duration_seconds": round(time.time() - self.start_time, 2)
        }
        
        return all_results
    
    def print_results(self, results: Dict[str, Any]):
        """Print test results in a formatted way"""
        print("\n" + "=" * 80)
        print("ğŸ† ULTIMATE MOE COMPLETE INTEGRATION TEST RESULTS")
        print("=" * 80)
        
        # Print summary
        summary = results["summary"]
        print(f"\nğŸ“Š OVERALL SUMMARY:")
        print(f"   Total Tests: {summary['total_tests']}")
        print(f"   Passed: {summary['total_passed']} âœ…")
        print(f"   Failed: {summary['total_failed']} âŒ")
        print(f"   Success Rate: {summary['success_rate']}%")
        print(f"   Total Latency: {summary['total_latency_ms']}ms")
        print(f"   Average Latency: {summary['average_latency_ms']}ms")
        print(f"   Total Errors: {summary['total_errors']}")
        print(f"   Test Duration: {summary['test_duration_seconds']}s")
        
        # Print phase results
        print(f"\nğŸ”§ PHASE RESULTS:")
        for phase_name, phase_results in results["phases"].items():
            if "integration_tests" in phase_results:
                phase_tests = len(phase_results["integration_tests"])
                phase_passed = len([t for t in phase_results["integration_tests"] if t["status"] == "PASS"])
                phase_failed = len([t for t in phase_results["integration_tests"] if t["status"] == "FAIL"])
                phase_success_rate = round((phase_passed / phase_tests) * 100, 2) if phase_tests > 0 else 0
                phase_errors = len(phase_results.get("errors", []))
                
                status_icon = "âœ…" if phase_success_rate >= 80 else "âš ï¸" if phase_success_rate >= 60 else "âŒ"
                print(f"   {status_icon} {phase_name.replace('_', ' ').title()}: {phase_success_rate}% ({phase_passed}/{phase_tests}) - {phase_errors} errors")
                
                # Print errors if any
                if phase_errors > 0:
                    for error in phase_results.get("errors", []):
                        print(f"      âŒ Error: {error}")
        
        # Print detailed test results
        print(f"\nğŸ“‹ DETAILED TEST RESULTS:")
        for phase_name, phase_results in results["phases"].items():
            if "integration_tests" in phase_results:
                print(f"\n   {phase_name.replace('_', ' ').title()}:")
                for test in phase_results["integration_tests"]:
                    status_icon = "âœ…" if test["status"] == "PASS" else "âŒ"
                    print(f"     {status_icon} {test['test']}: {test['latency']}ms")
        
        print("\n" + "=" * 80)
        print("ğŸ† COMPLETE INTEGRATION TESTING FINISHED")
        print("=" * 80)

async def main():
    """Main test runner"""
    tester = UltimateMoECompleteIntegrationTester()
    
    try:
        # Run complete integration test
        results = await tester.run_complete_integration_test()
        
        # Print results
        tester.print_results(results)
        
        # Save results to file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"complete_integration_test_results_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        
        print(f"\nğŸ’¾ Complete test results saved to: {filename}")
        
        # Return success/failure
        success_rate = results["summary"]["success_rate"]
        if success_rate >= 80:
            print(f"\nğŸ‰ Complete Integration Testing PASSED with {success_rate}% success rate!")
            print("ğŸš€ Ultimate MoE Solution is ready for production deployment!")
            return True
        else:
            print(f"\nâš ï¸ Complete Integration Testing needs improvement: {success_rate}% success rate")
            return False
            
    except Exception as e:
        print(f"\nâŒ Complete integration test suite failed with error: {e}")
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1) 
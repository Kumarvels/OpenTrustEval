{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11c0f846",
   "metadata": {},
   "source": [
    "# OpenTrustEval Pipeline Monitoring & Analytics Dashboard\n",
    "\n",
    "This notebook provides interactive monitoring, analytics, and SQL querying for the OpenTrustEval pipeline using the `pipeline_logs.db` database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5b20c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin Analytics & Real-Time Monitoring\n",
    "This section provides advanced analytics for each plugin and enables real-time monitoring of pipeline runs. Use the visualizations to compare plugin performance, detect anomalies, and monitor the system live. Uncomment the auto-refresh code to enable live updates.\n",
    "# Export Logs to CSV\n",
    "Export the pipeline logs to a CSV file for external analysis or reporting.\n",
    "logs_df = load_logs()\n",
    "logs_df.to_csv('pipeline_logs_export.csv', index=False)\n",
    "display(Markdown('Logs exported to <code>pipeline_logs_export.csv</code> in the workspace.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b495b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Logs to Excel and JSON\n",
    "Export the pipeline logs to Excel and JSON formats for further analysis or sharing.\n",
    "logs_df = load_logs()\n",
    "logs_df.to_excel('pipeline_logs_export.xlsx', index=False)\n",
    "logs_df.to_json('pipeline_logs_export.json', orient='records', lines=True)\n",
    "display(Markdown('Logs exported to <code>pipeline_logs_export.xlsx</code> and <code>pipeline_logs_export.json</code> in the workspace.'))\n",
    "# Email Alert for Recent Errors\n",
    "Send an email alert if errors are detected in the most recent pipeline runs. (Requires configuration of SMTP credentials.)\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_error_alert(email_to, email_from, smtp_server, smtp_port, smtp_user, smtp_pass, n=5):\n",
    "    recent = load_logs().sort_values('timestamp', ascending=False).head(n)\n",
    "    errors = recent[recent['error'].notnull()]\n",
    "    if not errors.empty:\n",
    "        msg = MIMEText(errors.to_string())\n",
    "        msg['Subject'] = f'OpenTrustEval Pipeline Error Alert ({len(errors)} recent errors)'\n",
    "        msg['From'] = email_from\n",
    "        msg['To'] = email_to\n",
    "        with smtplib.SMTP_SSL(smtp_server, smtp_port) as server:\n",
    "            server.login(smtp_user, smtp_pass)\n",
    "            server.sendmail(email_from, [email_to], msg.as_string())\n",
    "        print(f\"Alert sent to {email_to}\")\n",
    "    else:\n",
    "        print(\"No recent errors to alert.\")\n",
    "\n",
    "# Example usage (fill in credentials):\n",
    "# send_error_alert('to@example.com', 'from@example.com', 'smtp.example.com', 465, 'user', 'pass')\n",
    "# Integration with External Tools\n",
    "You can integrate pipeline logs with Slack, cloud dashboards, or monitoring systems. For example, use the Slack API to send alerts, or upload logs to a cloud storage bucket for BI dashboards. See documentation for code samples and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d202c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Logs to AWS S3\n",
    "Upload the exported logs to an S3 bucket for cloud storage or integration with BI dashboards. Requires `boto3` and AWS credentials.\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "def upload_to_s3(file_path, bucket, object_name=None, aws_access_key_id=None, aws_secret_access_key=None, region_name=None):\n",
    "    s3 = boto3.client('s3',\n",
    "                      aws_access_key_id=aws_access_key_id,\n",
    "                      aws_secret_access_key=aws_secret_access_key,\n",
    "                      region_name=region_name)\n",
    "    try:\n",
    "        s3.upload_file(file_path, bucket, object_name or file_path)\n",
    "        print(f\"Uploaded {file_path} to s3://{bucket}/{object_name or file_path}\")\n",
    "    except NoCredentialsError:\n",
    "        print(\"AWS credentials not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Upload failed: {e}\")\n",
    "\n",
    "# Example usage (fill in credentials and bucket):\n",
    "# upload_to_s3('pipeline_logs_export.csv', 'your-bucket', 'logs/pipeline_logs_export.csv', 'AKIA...', 'SECRET...', 'us-east-1')\n",
    "# Send Logs to Webhook Endpoint\n",
    "Send the exported logs to a webhook (e.g., for integration with monitoring or automation tools).\n",
    "import requests\n",
    "\n",
    "def send_logs_to_webhook(file_path, webhook_url):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        files = {'file': (file_path, f)}\n",
    "        response = requests.post(webhook_url, files=files)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Logs sent to webhook: {webhook_url}\")\n",
    "        else:\n",
    "            print(f\"Webhook upload failed: {response.status_code} {response.text}\")\n",
    "\n",
    "# Example usage (fill in your webhook URL):\n",
    "# send_logs_to_webhook('pipeline_logs_export.csv', 'https://your-webhook-endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbba8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload Logs to Google Cloud Storage\n",
    "Upload the exported logs to a Google Cloud Storage bucket. Requires `google-cloud-storage` and GCP credentials.\n",
    "from google.cloud import storage\n",
    "\n",
    "def upload_to_gcs(file_path, bucket_name, destination_blob_name, credentials_path=None):\n",
    "    if credentials_path:\n",
    "        storage_client = storage.Client.from_service_account_json(credentials_path)\n",
    "    else:\n",
    "        storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(file_path)\n",
    "    print(f\"Uploaded {file_path} to gs://{bucket_name}/{destination_blob_name}\")\n",
    "\n",
    "# Example usage (fill in credentials and bucket):\n",
    "# upload_to_gcs('pipeline_logs_export.csv', 'your-bucket', 'logs/pipeline_logs_export.csv', 'path/to/credentials.json')\n",
    "# Upload Logs to Azure Blob Storage\n",
    "Upload the exported logs to Azure Blob Storage. Requires `azure-storage-blob` and Azure credentials.\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "def upload_to_azure_blob(file_path, connection_string, container_name, blob_name):\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "    blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "    with open(file_path, 'rb') as data:\n",
    "        blob_client.upload_blob(data, overwrite=True)\n",
    "    print(f\"Uploaded {file_path} to Azure container {container_name} as {blob_name}\")\n",
    "\n",
    "# Example usage (fill in connection string and container):\n",
    "# upload_to_azure_blob('pipeline_logs_export.csv', 'your-connection-string', 'your-container', 'logs/pipeline_logs_export.csv')\n",
    "# Advanced Automation: Scheduled Exports\n",
    "Automate log exports and uploads using cron jobs, Airflow, or cloud-native schedulers. For example, create a script that runs the export/upload code and schedule it to run hourly or daily. See documentation for setup instructions and best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f057638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Next Step Functionality: Load logs and run SQL queries\n",
    "def load_logs(db_path='pipeline_logs.db'):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query('SELECT * FROM pipeline_logs', conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def run_sql_query(query, db_path='pipeline_logs.db'):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bc8307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline Logs Summary\n",
    "Display a summary of recent pipeline runs, including total runs, errors, and average timings.\n",
    "import numpy as np\n",
    "logs_df = load_logs()\n",
    "if not logs_df.empty:\n",
    "    total_runs = len(logs_df)\n",
    "    error_count = logs_df['error'].notnull().sum() if 'error' in logs_df.columns else 0\n",
    "    avg_time = logs_df['total_time'].mean() if 'total_time' in logs_df.columns else np.nan\n",
    "    display(Markdown(f\"**Total Runs:** {total_runs}  |  **Errors:** {error_count}  |  **Avg. Time (s):** {avg_time:.2f}\"))\n",
    "else:\n",
    "    display(Markdown(\"No logs found in the database.\"))\n",
    "# Visualize Pipeline Timings\n",
    "Visualize the distribution of total pipeline execution times.\n",
    "if not logs_df.empty and 'total_time' in logs_df.columns:\n",
    "    fig = px.histogram(logs_df, x='total_time', nbins=30, title='Pipeline Execution Time Distribution (seconds)')\n",
    "    fig.show()\n",
    "# Visualize Resource Usage\n",
    "Show CPU and memory usage per run if available.\n",
    "if not logs_df.empty and 'cpu_usage' in logs_df.columns and 'memory_usage' in logs_df.columns:\n",
    "    fig = px.scatter(logs_df, x='cpu_usage', y='memory_usage', color='total_time',\n",
    "                     title='CPU vs Memory Usage per Run', labels={'cpu_usage':'CPU (%)', 'memory_usage':'Memory (MB)'})\n",
    "    fig.show()\n",
    "# Error Rate Over Time\n",
    "Visualize error rate trends if error column exists.\n",
    "if not logs_df.empty and 'timestamp' in logs_df.columns and 'error' in logs_df.columns:\n",
    "    logs_df['date'] = pd.to_datetime(logs_df['timestamp']).dt.date\n",
    "    error_trend = logs_df.groupby('date')['error'].apply(lambda x: x.notnull().mean())\n",
    "    fig = px.line(error_trend, title='Error Rate Over Time')\n",
    "    fig.update_layout(xaxis_title='Date', yaxis_title='Error Rate')\n",
    "    fig.show()\n",
    "# Interactive SQL Query\n",
    "Run a custom SQL query on the pipeline logs database.\n",
    "import ipywidgets as widgets\n",
    "query_box = widgets.Textarea(value='SELECT * FROM pipeline_logs LIMIT 10', description='SQL Query:', layout=widgets.Layout(width='100%', height='80px'))\n",
    "run_button = widgets.Button(description='Run Query')\n",
    "output = widgets.Output()\n",
    "def on_run_query_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        try:\n",
    "            df = run_sql_query(query_box.value)\n",
    "            display(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "run_button.on_click(on_run_query_clicked)\n",
    "display(query_box, run_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8450cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plugin-Specific Results Breakdown\n",
    "Visualize and analyze results for each plugin (if plugin columns exist in logs).\n",
    "plugin_cols = [col for col in logs_df.columns if col.startswith('plugin_')]\n",
    "if plugin_cols:\n",
    "    for col in plugin_cols:\n",
    "        display(Markdown(f\"### Results for {col}\"))\n",
    "        if logs_df[col].dtype == 'float' or logs_df[col].dtype == 'int':\n",
    "            fig = px.histogram(logs_df, x=col, title=f'Distribution for {col}')\n",
    "            fig.show()\n",
    "        else:\n",
    "            value_counts = logs_df[col].value_counts()\n",
    "            fig = px.bar(x=value_counts.index, y=value_counts.values, labels={'x':col, 'y':'Count'}, title=f'Value Counts for {col}')\n",
    "            fig.show()\n",
    "else:\n",
    "    display(Markdown(\"No plugin-specific columns found in logs.\"))\n",
    "# Real-Time Log Monitoring (Experimental)\n",
    "Display the most recent pipeline runs and auto-refresh every 10 seconds (if running in a Jupyter environment).\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import threading\n",
    "\n",
    "def show_latest_logs(n=5):\n",
    "    latest = load_logs().sort_values('timestamp', ascending=False).head(n)\n",
    "    display(latest)\n",
    "\n",
    "def auto_refresh_logs(interval=10, n=5, stop_after=60):\n",
    "    start = time.time()\n",
    "    while time.time() - start < stop_after:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Showing latest {n} runs (auto-refreshing every {interval}s, stop after {stop_after}s)...\")\n",
    "        show_latest_logs(n)\n",
    "        time.sleep(interval)\n",
    "    print(\"Auto-refresh stopped.\")\n",
    "\n",
    "# Uncomment to enable auto-refresh (will run for 60 seconds):\n",
    "# auto_refresh_logs(interval=10, n=5, stop_after=60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

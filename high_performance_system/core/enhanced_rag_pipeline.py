"""
Enhanced RAG Pipeline for Ultimate MoE Solution

This module provides an advanced RAG (Retrieval-Augmented Generation) pipeline
with semantic processing, multi-modal embeddings, hybrid search, and MoE verification.
"""

import asyncio
import time
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from enum import Enum
import re
import json
from datetime import datetime
import hashlib


class ChunkingStrategy(Enum):
    """Chunking strategies for semantic processing"""
    SEMANTIC = "semantic"
    FIXED_SIZE = "fixed_size"
    SENTENCE_BASED = "sentence_based"
    PARAGRAPH_BASED = "paragraph_based"


class SearchStrategy(Enum):
    """Search strategies for retrieval"""
    DENSE = "dense"
    SPARSE = "sparse"
    HYBRID = "hybrid"
    SEMANTIC = "semantic"


@dataclass
class DocumentChunk:
    """Represents a chunk of document with metadata"""
    id: str
    text: str
    start_position: int
    end_position: int
    chunk_type: str
    metadata: Dict[str, Any]
    embedding: Optional[np.ndarray] = None
    relevance_score: float = 0.0


@dataclass
class SearchResult:
    """Represents a search result with metadata"""
    chunk: DocumentChunk
    similarity_score: float
    relevance_score: float
    source_document: str
    position_in_document: int
    context: str
    metadata: Dict[str, Any]


@dataclass
class RAGPipelineResult:
    """Result of enhanced RAG pipeline processing"""
    query: str
    chunks: List[DocumentChunk]
    search_results: List[SearchResult]
    reranked_results: List[SearchResult]
    verified_results: List[SearchResult]
    final_answer: str
    confidence: float
    metadata: Dict[str, Any]
    processing_time: float


class AdvancedSemanticChunker:
    """Advanced semantic chunking with multiple strategies"""
    
    def __init__(self, strategy: ChunkingStrategy = ChunkingStrategy.SEMANTIC):
        self.strategy = strategy
        self.min_chunk_size = 100
        self.max_chunk_size = 1000
        self.overlap_size = 50
        
        # Semantic indicators for chunking
        self.semantic_boundaries = [
            r'\n\n+',  # Multiple newlines
            r'\.\s+[A-Z]',  # Sentence boundaries
            r'[.!?]\s+[A-Z]',  # Punctuation followed by capital
            r'\n[A-Z][a-z]+:',  # Headers
            r'\n\d+\.\s+',  # Numbered lists
            r'\nâ€¢\s+',  # Bullet points
        ]
    
    async def advanced_chunking(self, documents: List[str]) -> List[DocumentChunk]:
        """Advanced semantic chunking of documents"""
        all_chunks = []
        
        for doc_idx, document in enumerate(documents):
            if self.strategy == ChunkingStrategy.SEMANTIC:
                chunks = await self._semantic_chunking(document, doc_idx)
            elif self.strategy == ChunkingStrategy.FIXED_SIZE:
                chunks = await self._fixed_size_chunking(document, doc_idx)
            elif self.strategy == ChunkingStrategy.SENTENCE_BASED:
                chunks = await self._sentence_based_chunking(document, doc_idx)
            elif self.strategy == ChunkingStrategy.PARAGRAPH_BASED:
                chunks = await self._paragraph_based_chunking(document, doc_idx)
            else:
                chunks = await self._semantic_chunking(document, doc_idx)
            
            all_chunks.extend(chunks)
        
        return all_chunks
    
    async def _semantic_chunking(self, document: str, doc_idx: int) -> List[DocumentChunk]:
        """Semantic chunking based on content boundaries"""
        chunks = []
        current_pos = 0
        
        # Find semantic boundaries
        boundaries = []
        for pattern in self.semantic_boundaries:
            matches = list(re.finditer(pattern, document))
            boundaries.extend([(match.start(), match.end()) for match in matches])
        
        # Sort boundaries by position
        boundaries.sort(key=lambda x: x[0])
        
        # Create chunks based on boundaries
        for i, (start, end) in enumerate(boundaries):
            if start - current_pos >= self.min_chunk_size:
                chunk_text = document[current_pos:start].strip()
                if chunk_text:
                    chunk = DocumentChunk(
                        id=f"doc_{doc_idx}_chunk_{i}",
                        text=chunk_text,
                        start_position=current_pos,
                        end_position=start,
                        chunk_type="semantic",
                        metadata={
                            "document_index": doc_idx,
                            "boundary_type": "semantic",
                            "chunk_index": i
                        }
                    )
                    chunks.append(chunk)
                current_pos = start
        
        # Handle remaining text
        if current_pos < len(document):
            remaining_text = document[current_pos:].strip()
            if remaining_text and len(remaining_text) >= self.min_chunk_size:
                chunk = DocumentChunk(
                    id=f"doc_{doc_idx}_chunk_final",
                    text=remaining_text,
                    start_position=current_pos,
                    end_position=len(document),
                    chunk_type="semantic",
                    metadata={
                        "document_index": doc_idx,
                        "boundary_type": "final",
                        "chunk_index": len(chunks)
                    }
                )
                chunks.append(chunk)
        
        return chunks
    
    async def _fixed_size_chunking(self, document: str, doc_idx: int) -> List[DocumentChunk]:
        """Fixed-size chunking with overlap"""
        chunks = []
        current_pos = 0
        chunk_idx = 0
        
        while current_pos < len(document):
            end_pos = min(current_pos + self.max_chunk_size, len(document))
            
            # Try to break at sentence boundary
            if end_pos < len(document):
                # Look for sentence boundary in the last 100 characters
                search_start = max(current_pos + self.max_chunk_size - 100, current_pos)
                for i in range(search_start, end_pos):
                    if document[i] in '.!?':
                        end_pos = i + 1
                        break
            
            chunk_text = document[current_pos:end_pos].strip()
            if chunk_text:
                chunk = DocumentChunk(
                    id=f"doc_{doc_idx}_chunk_{chunk_idx}",
                    text=chunk_text,
                    start_position=current_pos,
                    end_position=end_pos,
                    chunk_type="fixed_size",
                    metadata={
                        "document_index": doc_idx,
                        "chunk_size": len(chunk_text),
                        "chunk_index": chunk_idx
                    }
                )
                chunks.append(chunk)
            
            current_pos = max(current_pos + 1, end_pos - self.overlap_size)
            chunk_idx += 1
        
        return chunks
    
    async def _sentence_based_chunking(self, document: str, doc_idx: int) -> List[DocumentChunk]:
        """Sentence-based chunking"""
        sentences = re.split(r'[.!?]+', document)
        chunks = []
        current_chunk = ""
        chunk_idx = 0
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            if len(current_chunk) + len(sentence) <= self.max_chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunk = DocumentChunk(
                        id=f"doc_{doc_idx}_chunk_{chunk_idx}",
                        text=current_chunk.strip(),
                        start_position=0,  # Simplified for sentence-based
                        end_position=len(current_chunk),
                        chunk_type="sentence_based",
                        metadata={
                            "document_index": doc_idx,
                            "sentence_count": current_chunk.count('.'),
                            "chunk_index": chunk_idx
                        }
                    )
                    chunks.append(chunk)
                    chunk_idx += 1
                
                current_chunk = sentence + ". "
        
        # Add final chunk
        if current_chunk:
            chunk = DocumentChunk(
                id=f"doc_{doc_idx}_chunk_{chunk_idx}",
                text=current_chunk.strip(),
                start_position=0,
                end_position=len(current_chunk),
                chunk_type="sentence_based",
                metadata={
                    "document_index": doc_idx,
                    "sentence_count": current_chunk.count('.'),
                    "chunk_index": chunk_idx
                }
            )
            chunks.append(chunk)
        
        return chunks
    
    async def _paragraph_based_chunking(self, document: str, doc_idx: int) -> List[DocumentChunk]:
        """Paragraph-based chunking"""
        paragraphs = document.split('\n\n')
        chunks = []
        
        for i, paragraph in enumerate(paragraphs):
            paragraph = paragraph.strip()
            if not paragraph:
                continue
            
            chunk = DocumentChunk(
                id=f"doc_{doc_idx}_chunk_{i}",
                text=paragraph,
                start_position=0,  # Simplified for paragraph-based
                end_position=len(paragraph),
                chunk_type="paragraph_based",
                metadata={
                    "document_index": doc_idx,
                    "paragraph_index": i,
                    "chunk_index": i
                }
            )
            chunks.append(chunk)
        
        return chunks


class MultiModalEmbeddingGenerator:
    """Multi-modal embedding generation for different content types"""
    
    def __init__(self):
        self.embedding_dimensions = {
            'text': 768,
            'code': 512,
            'table': 256,
            'image': 1024
        }
        
        # Simple hash-based embedding for demonstration
        # In production, this would use actual embedding models
        self.embedding_cache = {}
    
    async def generate_embeddings(self, chunks: List[DocumentChunk]) -> List[DocumentChunk]:
        """Generate embeddings for document chunks"""
        for chunk in chunks:
            if chunk.id not in self.embedding_cache:
                # Generate embedding based on content type
                embedding = await self._generate_chunk_embedding(chunk)
                self.embedding_cache[chunk.id] = embedding
            
            chunk.embedding = self.embedding_cache[chunk.id]
        
        return chunks
    
    async def _generate_chunk_embedding(self, chunk: DocumentChunk) -> np.ndarray:
        """Generate embedding for a single chunk"""
        # Determine content type
        content_type = self._detect_content_type(chunk.text)
        
        # Generate hash-based embedding (simplified for demonstration)
        text_hash = hashlib.md5(chunk.text.encode()).hexdigest()
        hash_int = int(text_hash[:8], 16)
        
        # Create embedding vector
        dim = self.embedding_dimensions.get(content_type, 768)
        embedding = np.zeros(dim)
        
        # Fill embedding based on hash
        for i in range(min(dim, 8)):
            embedding[i] = (hash_int >> (i * 4)) & 0xF
        
        # Normalize embedding
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    def _detect_content_type(self, text: str) -> str:
        """Detect content type of text"""
        # Simple heuristics for content type detection
        if re.search(r'def\s+\w+\s*\(|class\s+\w+|import\s+\w+', text):
            return 'code'
        elif re.search(r'\|\s*\w+\s*\|', text) or '\t' in text:
            return 'table'
        elif re.search(r'<img|\.jpg|\.png|\.gif', text, re.IGNORECASE):
            return 'image'
        else:
            return 'text'


class DistributedVectorStore:
    """Distributed vector store for scalable storage and retrieval"""
    
    def __init__(self):
        self.vectors = {}
        self.metadata = {}
        self.index = {}  # Simple index for demonstration
        
    async def store_vectors(self, chunks: List[DocumentChunk]):
        """Store vectors in the distributed store"""
        for chunk in chunks:
            if chunk.embedding is not None:
                self.vectors[chunk.id] = chunk.embedding
                self.metadata[chunk.id] = chunk.metadata
                
                # Create simple index
                words = chunk.text.lower().split()
                for word in words:
                    if word not in self.index:
                        self.index[word] = []
                    if chunk.id not in self.index[word]:
                        self.index[word].append(chunk.id)
    
    async def search_vectors(self, query_embedding: np.ndarray, top_k: int = 10) -> List[Tuple[str, float]]:
        """Search vectors using cosine similarity"""
        results = []
        
        for chunk_id, vector in self.vectors.items():
            similarity = np.dot(query_embedding, vector)
            results.append((chunk_id, similarity))
        
        # Sort by similarity and return top_k
        results.sort(key=lambda x: x[1], reverse=True)
        return results[:top_k]
    
    async def get_chunk_by_id(self, chunk_id: str) -> Optional[DocumentChunk]:
        """Get chunk by ID"""
        if chunk_id in self.vectors:
            return DocumentChunk(
                id=chunk_id,
                text="",  # Would need to store text separately
                start_position=0,
                end_position=0,
                chunk_type="retrieved",
                metadata=self.metadata.get(chunk_id, {}),
                embedding=self.vectors[chunk_id]
            )
        return None


class AdvancedHybridSearch:
    """Advanced hybrid search combining multiple search strategies"""
    
    def __init__(self, vector_store: DistributedVectorStore):
        self.vector_store = vector_store
        self.search_weights = {
            SearchStrategy.DENSE: 0.6,
            SearchStrategy.SPARSE: 0.3,
            SearchStrategy.SEMANTIC: 0.1
        }
    
    async def search(self, query: str, embeddings: List[DocumentChunk]) -> List[SearchResult]:
        """Hybrid search combining multiple strategies"""
        # Generate query embedding
        query_embedding = await self._generate_query_embedding(query)
        
        # Perform different search strategies
        dense_results = await self._dense_search(query_embedding)
        sparse_results = await self._sparse_search(query)
        semantic_results = await self._semantic_search(query, embeddings)
        
        # Combine results
        combined_results = await self._combine_search_results(
            dense_results, sparse_results, semantic_results
        )
        
        return combined_results
    
    async def _generate_query_embedding(self, query: str) -> np.ndarray:
        """Generate embedding for query"""
        # Simplified query embedding generation
        query_hash = hashlib.md5(query.encode()).hexdigest()
        query_int = int(query_hash[:8], 16)
        
        embedding = np.zeros(768)
        for i in range(min(768, 8)):
            embedding[i] = (query_int >> (i * 4)) & 0xF
        
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
    
    async def _dense_search(self, query_embedding: np.ndarray) -> List[SearchResult]:
        """Dense vector search"""
        vector_results = await self.vector_store.search_vectors(query_embedding, top_k=20)
        
        results = []
        for chunk_id, similarity in vector_results:
            chunk = await self.vector_store.get_chunk_by_id(chunk_id)
            if chunk:
                result = SearchResult(
                    chunk=chunk,
                    similarity_score=similarity,
                    relevance_score=similarity * self.search_weights[SearchStrategy.DENSE],
                    source_document=f"doc_{chunk.metadata.get('document_index', 0)}",
                    position_in_document=chunk.start_position,
                    context=chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                    metadata={"search_strategy": "dense"}
                )
                results.append(result)
        
        return results
    
    async def _sparse_search(self, query: str) -> List[SearchResult]:
        """Sparse keyword search"""
        query_words = query.lower().split()
        results = []
        
        # Simple keyword matching
        for chunk_id, metadata in self.vector_store.metadata.items():
            chunk = await self.vector_store.get_chunk_by_id(chunk_id)
            if not chunk:
                continue
            
            # Count matching words
            chunk_words = chunk.text.lower().split()
            matches = sum(1 for word in query_words if word in chunk_words)
            
            if matches > 0:
                relevance = matches / len(query_words) * self.search_weights[SearchStrategy.SPARSE]
                result = SearchResult(
                    chunk=chunk,
                    similarity_score=relevance,
                    relevance_score=relevance,
                    source_document=f"doc_{metadata.get('document_index', 0)}",
                    position_in_document=chunk.start_position,
                    context=chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                    metadata={"search_strategy": "sparse", "matches": matches}
                )
                results.append(result)
        
        return results
    
    async def _semantic_search(self, query: str, embeddings: List[DocumentChunk]) -> List[SearchResult]:
        """Semantic search based on content similarity"""
        query_embedding = await self._generate_query_embedding(query)
        results = []
        
        for chunk in embeddings:
            if chunk.embedding is not None:
                similarity = np.dot(query_embedding, chunk.embedding)
                relevance = similarity * self.search_weights[SearchStrategy.SEMANTIC]
                
                result = SearchResult(
                    chunk=chunk,
                    similarity_score=similarity,
                    relevance_score=relevance,
                    source_document=f"doc_{chunk.metadata.get('document_index', 0)}",
                    position_in_document=chunk.start_position,
                    context=chunk.text[:200] + "..." if len(chunk.text) > 200 else chunk.text,
                    metadata={"search_strategy": "semantic"}
                )
                results.append(result)
        
        return results
    
    async def _combine_search_results(self, dense_results: List[SearchResult], 
                                    sparse_results: List[SearchResult],
                                    semantic_results: List[SearchResult]) -> List[SearchResult]:
        """Combine results from different search strategies"""
        # Create a dictionary to combine results by chunk ID
        combined = {}
        
        # Add dense results
        for result in dense_results:
            combined[result.chunk.id] = result
        
        # Add sparse results
        for result in sparse_results:
            if result.chunk.id in combined:
                combined[result.chunk.id].relevance_score += result.relevance_score
            else:
                combined[result.chunk.id] = result
        
        # Add semantic results
        for result in semantic_results:
            if result.chunk.id in combined:
                combined[result.chunk.id].relevance_score += result.relevance_score
            else:
                combined[result.chunk.id] = result
        
        # Sort by combined relevance score
        combined_list = list(combined.values())
        combined_list.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return combined_list[:20]  # Return top 20 results


class IntelligentContextReranker:
    """Intelligent context re-ranking for search results"""
    
    def __init__(self):
        self.reranking_factors = {
            'relevance': 0.4,
            'freshness': 0.2,
            'authority': 0.2,
            'coherence': 0.1,
            'diversity': 0.1
        }
    
    async def rerank(self, query: str, search_results: List[SearchResult]) -> List[SearchResult]:
        """Intelligent re-ranking of search results"""
        if not search_results:
            return []
        
        # Calculate reranking scores
        for result in search_results:
            rerank_score = await self._calculate_rerank_score(query, result, search_results)
            result.relevance_score = rerank_score
        
        # Sort by reranked score
        reranked_results = sorted(search_results, key=lambda x: x.relevance_score, reverse=True)
        
        return reranked_results
    
    async def _calculate_rerank_score(self, query: str, result: SearchResult, 
                                    all_results: List[SearchResult]) -> float:
        """Calculate reranking score for a result"""
        # Relevance score (already calculated)
        relevance_score = result.similarity_score
        
        # Freshness score (based on document metadata)
        freshness_score = self._calculate_freshness_score(result)
        
        # Authority score (based on source quality)
        authority_score = self._calculate_authority_score(result)
        
        # Coherence score (based on text quality)
        coherence_score = self._calculate_coherence_score(result)
        
        # Diversity score (based on result diversity)
        diversity_score = self._calculate_diversity_score(result, all_results)
        
        # Combine scores
        final_score = (
            relevance_score * self.reranking_factors['relevance'] +
            freshness_score * self.reranking_factors['freshness'] +
            authority_score * self.reranking_factors['authority'] +
            coherence_score * self.reranking_factors['coherence'] +
            diversity_score * self.reranking_factors['diversity']
        )
        
        return final_score
    
    def _calculate_freshness_score(self, result: SearchResult) -> float:
        """Calculate freshness score"""
        # Simplified freshness calculation
        # In production, this would use actual timestamps
        return 0.8  # Default freshness score
    
    def _calculate_authority_score(self, result: SearchResult) -> float:
        """Calculate authority score"""
        text = result.chunk.text.lower()
        
        # Authority indicators
        authority_indicators = [
            'research', 'study', 'analysis', 'report', 'official',
            'government', 'university', 'institution', 'expert'
        ]
        
        authority_count = sum(1 for indicator in authority_indicators if indicator in text)
        return min(1.0, authority_count * 0.2)
    
    def _calculate_coherence_score(self, result: SearchResult) -> float:
        """Calculate coherence score"""
        text = result.chunk.text
        
        # Simple coherence indicators
        sentence_count = text.count('.') + text.count('!') + text.count('?')
        word_count = len(text.split())
        
        if word_count == 0:
            return 0.0
        
        # Coherence based on sentence length and structure
        avg_sentence_length = word_count / max(1, sentence_count)
        
        # Prefer sentences of reasonable length (10-30 words)
        if 10 <= avg_sentence_length <= 30:
            return 1.0
        elif 5 <= avg_sentence_length <= 50:
            return 0.7
        else:
            return 0.3
    
    def _calculate_diversity_score(self, result: SearchResult, all_results: List[SearchResult]) -> float:
        """Calculate diversity score"""
        # Simplified diversity calculation
        # In production, this would measure content diversity
        return 0.9  # Default diversity score


class MoEDomainVerifier:
    """MoE domain verification for RAG results"""
    
    def __init__(self):
        # Simplified MoE verification
        # In production, this would integrate with the actual MoE system
        self.domain_keywords = {
            'ecommerce': ['product', 'price', 'sale', 'customer', 'order'],
            'healthcare': ['medical', 'patient', 'treatment', 'doctor', 'health'],
            'finance': ['money', 'bank', 'investment', 'financial', 'market'],
            'technology': ['software', 'computer', 'digital', 'tech', 'system']
        }
    
    async def verify_text(self, text: str) -> Dict[str, Any]:
        """Verify text using MoE approach"""
        text_lower = text.lower()
        
        # Detect domain
        domain_scores = {}
        for domain, keywords in self.domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            domain_scores[domain] = score / len(keywords)
        
        # Find primary domain
        primary_domain = max(domain_scores.items(), key=lambda x: x[1])[0]
        
        # Calculate verification score
        verification_score = domain_scores[primary_domain]
        
        return {
            'verification_score': verification_score,
            'primary_domain': primary_domain,
            'domain_scores': domain_scores,
            'confidence': min(1.0, verification_score * 1.5)
        }


class EnhancedRAGPipeline:
    """Enhanced RAG pipeline with all advanced features"""
    
    def __init__(self):
        # Core components
        self.semantic_chunker = AdvancedSemanticChunker()
        self.embedding_generator = MultiModalEmbeddingGenerator()
        self.vector_store = DistributedVectorStore()
        self.hybrid_search = AdvancedHybridSearch(self.vector_store)
        self.context_reranker = IntelligentContextReranker()
        self.moe_verifier = MoEDomainVerifier()
        
        # Performance tracking
        self.pipeline_stats = {
            'total_queries': 0,
            'average_processing_time': 0.0,
            'chunk_counts': [],
            'search_result_counts': []
        }
    
    async def enhanced_rag_with_moe(self, query: str, documents: List[str]) -> RAGPipelineResult:
        """Complete RAG pipeline with MoE verification"""
        start_time = time.time()
        
        # Step 1: Advanced semantic processing
        chunks = await self.semantic_chunker.advanced_chunking(documents)
        chunks_with_embeddings = await self.embedding_generator.generate_embeddings(chunks)
        
        # Step 2: Store vectors
        await self.vector_store.store_vectors(chunks_with_embeddings)
        
        # Step 3: Intelligent search and retrieval
        search_results = await self.hybrid_search.search(query, chunks_with_embeddings)
        
        # Step 4: Context re-ranking
        reranked_results = await self.context_reranker.rerank(query, search_results)
        
        # Step 5: MoE verification for each result
        verified_results = []
        for result in reranked_results:
            moe_verification = await self.moe_verifier.verify_text(result.chunk.text)
            result.metadata['moe_verification'] = moe_verification
            verified_results.append(result)
        
        # Step 6: Generate final answer
        final_answer = await self._generate_final_answer(query, verified_results)
        
        # Step 7: Calculate confidence
        confidence = self._calculate_confidence(verified_results)
        
        # Step 8: Update statistics
        processing_time = time.time() - start_time
        self._update_stats(len(chunks), len(search_results), processing_time)
        
        return RAGPipelineResult(
            query=query,
            chunks=chunks,
            search_results=search_results,
            reranked_results=reranked_results,
            verified_results=verified_results,
            final_answer=final_answer,
            confidence=confidence,
            metadata={
                'processing_time': processing_time,
                'chunk_count': len(chunks),
                'search_result_count': len(search_results),
                'reranked_count': len(reranked_results),
                'verified_count': len(verified_results)
            },
            processing_time=processing_time
        )
    
    async def _generate_final_answer(self, query: str, verified_results: List[SearchResult]) -> str:
        """Generate final answer from verified results"""
        if not verified_results:
            return "No relevant information found."
        
        # Combine top results
        top_results = verified_results[:3]
        combined_text = " ".join([result.chunk.text for result in top_results])
        
        # Simple answer generation (in production, this would use an LLM)
        answer = f"Based on the search results: {combined_text[:500]}..."
        
        return answer
    
    def _calculate_confidence(self, verified_results: List[SearchResult]) -> float:
        """Calculate overall confidence score"""
        if not verified_results:
            return 0.0
        
        # Calculate confidence based on verification scores and relevance
        total_confidence = 0.0
        total_weight = 0.0
        
        for i, result in enumerate(verified_results):
            weight = 1.0 / (i + 1)  # Higher weight for top results
            moe_verification = result.metadata.get('moe_verification', {})
            verification_score = moe_verification.get('verification_score', 0.0)
            
            confidence = (verification_score + result.relevance_score) / 2
            total_confidence += confidence * weight
            total_weight += weight
        
        return total_confidence / total_weight if total_weight > 0 else 0.0
    
    def _update_stats(self, chunk_count: int, search_result_count: int, processing_time: float):
        """Update pipeline statistics"""
        self.pipeline_stats['total_queries'] += 1
        self.pipeline_stats['chunk_counts'].append(chunk_count)
        self.pipeline_stats['search_result_counts'].append(search_result_count)
        
        # Update average processing time
        current_avg = self.pipeline_stats['average_processing_time']
        total_queries = self.pipeline_stats['total_queries']
        self.pipeline_stats['average_processing_time'] = (
            (current_avg * (total_queries - 1) + processing_time) / total_queries
        )
    
    def get_pipeline_stats(self) -> Dict[str, Any]:
        """Get pipeline performance statistics"""
        total_queries = self.pipeline_stats['total_queries']
        if total_queries == 0:
            return self.pipeline_stats
        
        return {
            'total_queries': total_queries,
            'average_processing_time': self.pipeline_stats['average_processing_time'],
            'average_chunk_count': sum(self.pipeline_stats['chunk_counts']) / len(self.pipeline_stats['chunk_counts']),
            'average_search_results': sum(self.pipeline_stats['search_result_counts']) / len(self.pipeline_stats['search_result_counts']),
            'pipeline_efficiency': total_queries / max(1, self.pipeline_stats['average_processing_time'])
        }


# Example usage and testing
async def test_enhanced_rag_pipeline():
    """Test the enhanced RAG pipeline"""
    pipeline = EnhancedRAGPipeline()
    
    # Test documents
    documents = [
        "The new iPhone 15 Pro features a titanium design and A17 Pro chip. Available for pre-order starting at $999. The device includes advanced camera capabilities and improved battery life.",
        "According to peer-reviewed research published in Nature in 2023, climate change has accelerated significantly. The study analyzed data from over 1000 weather stations worldwide.",
        "The Federal Reserve raised interest rates by 0.25% to combat inflation. This decision was based on economic data showing persistent price pressures in the economy."
    ]
    
    # Test query
    query = "What are the latest technology developments?"
    
    print("=== Testing Enhanced RAG Pipeline ===")
    print(f"Query: {query}")
    print(f"Documents: {len(documents)}")
    
    result = await pipeline.enhanced_rag_with_moe(query, documents)
    
    print(f"\nProcessing Time: {result.processing_time:.3f}s")
    print(f"Chunks Generated: {len(result.chunks)}")
    print(f"Search Results: {len(result.search_results)}")
    print(f"Reranked Results: {len(result.reranked_results)}")
    print(f"Verified Results: {len(result.verified_results)}")
    print(f"Confidence: {result.confidence:.3f}")
    
    print(f"\nFinal Answer: {result.final_answer}")
    
    print("\nTop Search Results:")
    for i, result_item in enumerate(result.verified_results[:3]):
        print(f"{i+1}. Score: {result_item.relevance_score:.3f}")
        print(f"   Text: {result_item.chunk.text[:100]}...")
        print(f"   MoE Verification: {result_item.metadata.get('moe_verification', {}).get('verification_score', 0):.3f}")
    
    print("\nPipeline Statistics:")
    stats = pipeline.get_pipeline_stats()
    for key, value in stats.items():
        print(f"{key}: {value}")


if __name__ == "__main__":
    asyncio.run(test_enhanced_rag_pipeline()) 